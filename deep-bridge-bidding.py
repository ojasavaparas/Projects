# -*- coding: utf-8 -*-
"""Copy of Project_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIKQJ69_V4PsWSqZ2W7L2FaO1iVZvd1y
"""

# Creating generator to load data in memory with lazy evaluation

import json
import os
import random

import matplotlib.pylab as plt
import numpy as np

from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TerminateOnNaN
from keras.layers import Input, Layer, Conv2D, MaxPooling2D, ZeroPadding2D, BatchNormalization
from keras.layers.core import Flatten, Dense, Dropout, Lambda, Activation
from keras.models import Sequential, Model, load_model
from keras.optimizers import RMSprop, Adam, SGD
from keras.preprocessing.image import ImageDataGenerator
from keras import layers

import tensorflow as tf


class LossHistory(Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.val_losses = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))


class DataFetcher:
    def __init__(self, path):
        random.seed(0)

        train_path = os.path.join(path, 'training')
        valid_path = os.path.join(path, 'validation')
        test_path = os.path.join(path, 'test')

        self.train_files = self._get_files(train_path)
        self.valid_files = self._get_files(valid_path)
        self.test_files = self._get_files(test_path)

        self.training_size = 7607530 # number of training samples
        self.validation_size = 1079897 # number of validation samples
        self.test_size = 2168309 # number of test samples

    def _get_files(self, dir):
        return [os.path.join(dir, f) for f in os.listdir(dir) if f.endswith('.json')]

    def load_file(self, path):
        with open(path) as f:
            data = json.load(f)
        return data

    def is_inconsistent_sample(self, data, sample_index):
        return (len(data['X_enn'][sample_index]) != 372 or
            len(data['Y_enn'][sample_index]) != 52 or
            len(data['X_pnn'][sample_index]) != 424 or
            len(data['Y_pnn'][sample_index]) != 38)

    def generate_batch(self, batch_type='training', data_type='enn', batch_size=32):
        if batch_type not in ['training', 'validation']:
            raise ValueError('batch_type must be "training" or "validation"')

        if data_type not in ['enn', 'pnn']:
            raise ValueError('data_type must be either "enn" or "pnn"')

        buffer = {
            'X_enn': [],
            'Y_enn': [],
            'X_pnn': [],
            'Y_pnn': [],
        }

        if batch_type == 'training':
            files = self.train_files
        elif batch_type == 'validation':
            files = self.valid_files

        while True:
            random.shuffle(files)

            for file in files:
                data = self.load_file(file)
                num_samples = len(data['X_enn'])

                for i in range(num_samples):
                    if self.is_inconsistent_sample(data, i):
                        continue

                    buffer['X_enn'].append(data['X_enn'][i])
                    buffer['Y_enn'].append(data['Y_enn'][i])
                    buffer['X_pnn'].append(data['X_pnn'][i])
                    buffer['Y_pnn'].append(data['Y_pnn'][i])

                    if len(buffer['X_enn']) == batch_size:
                        if data_type == 'enn':
                            X = np.array(buffer['X_enn']).reshape([-1, 372])
                            Y = np.array(buffer['Y_enn']).reshape([-1, 52])
                        elif data_type == 'pnn':
                            X = np.array(buffer['X_pnn']).reshape([-1, 424])
                            Y = np.array(buffer['Y_pnn']).reshape([-1, 38])

                        yield X, Y

                        buffer = {
                            'X_enn': [],
                            'Y_enn': [],
                            'X_pnn': [],
                            'Y_pnn': [],
                        }

            if buffer['X_enn']:
                if data_type == 'enn':
                    X = np.array(buffer['X_enn']).reshape([-1, 372])
                    Y = np.array(buffer['Y_enn']).reshape([-1, 52])
                elif data_type == 'pnn':
                    X = np.array(buffer['X_pnn']).reshape([-1, 424])
                    Y = np.array(buffer['Y_pnn']).reshape([-1, 38])

                yield X, Y


            buffer = {
                'X_enn': [],
                'Y_enn': [],
                'X_pnn': [],
                'Y_pnn': [],
            }

    def generate_test_batch(self, data_type='enn'):
        if data_type not in ['enn', 'pnn']:
            raise ValueError('data_type must be either "enn" or "pnn"')

        files = sorted(self.test_files)

        for file in files:
            data = self.load_file(file)
            num_samples = len(data['X_enn'])

            for i in range(num_samples):
                if self.is_inconsistent_sample(data, i):
                    continue

                if data_type == 'enn':
                    X = np.array(data['X_enn'][i]).reshape([-1, 372])
                elif data_type == 'pnn':
                    X = np.array(data['X_pnn'][i]).reshape([-1, 424])

                yield X

    def generate_from_file(self, filename, data_type='enn'):
        if data_type not in ['enn', 'pnn']:
            raise ValueError('data_type must be either "enn" or "pnn"')

        data = self.load_file(filename)
        num_samples = len(data['X_enn'])

        for i in range(num_samples):
            if self.is_inconsistent_sample(data, i):
                    continue

            if data_type == 'enn':
                X = np.array(data['X_enn'][i]).reshape([-1, 372])
            elif data_type == 'pnn':
                X = np.array(data['X_pnn'][i]).reshape([-1, 424])

            yield X

    def count_samples(self, data):
        num_samples = 0
        for i in range(len(data['X_enn'])):
            if self.is_inconsistent_sample(data, i):
                continue
            num_samples += 1
        return num_samples


def create_enn_model():
    input_ = Input(shape=[372])
    x = input_
    skip = x

    for i in range(4):
        x = layers.Dense(1500, activation = "relu")(x)
        x = layers.Dense(1500, activation = "relu")(x)
        x = layers.concatenate([skip, x])
        skip = x

    out = layers.Dense(52, activation = "sigmoid")(x)
    model = Model(inputs=input_, outputs=out)
    model.summary()

    model.compile(loss='binary_crossentropy', optimizer=Adam())
    return model

def create_pnn_model():
    input_ = Input(shape=[424])
    x = input_
    skip = x

    for i in range(5):
        x = layers.Dense(1200, activation = "relu")(x)
        x = layers.Dense(1200, activation = "relu")(x)
        x = layers.concatenate([skip, x])
        skip = x

    out = layers.Dense(38, activation = "sigmoid")(x)
    model = Model(inputs=input_, outputs=out)
    model.summary()

    model.compile(loss='binary_crossentropy', optimizer=Adam())
    return model

def train(fetcher, model, model_type='enn', model_filename='model_enn.hdf5', batch_size=32, epochs=3):
    if model_type not in ['enn', 'pnn']:
        raise ValueError('model_type must be either "enn" or "pnn"')

    callbacks = [
        LossHistory(),
        ModelCheckpoint(filepath=model_filename.format(model_type), verbose=1, save_best_only=True),
        EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto'),
        TerminateOnNaN()
    ]

    model.fit_generator(
        fetcher.generate_batch(batch_type='training', data_type=model_type, batch_size=batch_size),
        validation_data=fetcher.generate_batch(batch_type='validation', data_type=model_type, batch_size=batch_size),
        steps_per_epoch=np.ceil(fetcher.training_size/batch_size),
        validation_steps=np.ceil(fetcher.validation_size/batch_size),
        epochs=epochs,
        verbose=2, # Print one line per epoch
        callbacks=callbacks,
    )

def test(fetcher, model, model_type='enn', model_filename='model_enn.hdf5', batch_size=32):
    if model_type not in ['enn', 'pnn']:
        raise ValueError('model_type must be either "enn" or "pnn"')

    total = 0
    correct = 0

    for i, file in enumerate(fetcher.test_files):
        with open(file) as f:
            data = json.load(f)

        if model_type == 'enn':
            Y = np.array(data['Y_enn']).reshape([-1, 52])
            expected = convert_enn_output_to_symbol(Y)
        elif model_type == 'pnn':
            Y = np.array(data['Y_pnn']).reshape([-1, 38])
            expected = convert_pnn_output_to_symbol(Y)

        output = predict(fetcher, file, model, model_type=model_type, model_filename=model_filename, batch_size=batch_size)

        for (y_expected, y_output) in zip(expected, output):
            if model_type == 'enn':
                correct += len(set(y_expected).intersection(set(y_output)))
                total += len(y_expected)
            elif model_type == 'pnn':
                if y_expected == y_output:
                    correct += 1
                total += 1

        if i % 10 == 0:
            print('(%d/%d) - Accuracy: %.2f%% (%d/%d)' % (i+1, len(fetcher.test_files), 100*correct/total, correct, total))

    print('Accuracy: %.2f%% (%d/%d)' % (100*correct/total, correct, total))

def predict(fetcher, file_path, model, model_type='enn', model_filename='model_enn.hdf5', batch_size=32):
    if model_type not in ['enn', 'pnn']:
        raise ValueError('model_type must be either "enn" or "pnn"')

    with open(file_path) as f:
        data = json.load(f)

    num_samples = fetcher.count_samples(data)

    predictions = model.predict_generator(
        fetcher.generate_from_file(file_path, data_type=model_type),
        steps=num_samples,
        max_queue_size=batch_size,
    )

    if model_type == 'enn':
        result = convert_enn_output_to_symbol(predictions)
    elif model_type == 'pnn':
        # TODO: Sample from it
        result = convert_pnn_output_to_symbol(predictions)

    return result

def convert_enn_output_to_symbol(enn_output):
    suit_order = ['♠', '♥', '♦', '♣']
    value_order = ['A', 'K', 'Q', 'J', '10', '9', '8', '7', '6', '5', '4', '3', '2']
    cards = []

    for value in value_order:
        for suit in suit_order:
            cards.append(suit + value)

    result = []

    for output in enn_output:
        assert len(output) == 52
        cards_probabilities = list(zip(output, cards))
        cards_probabilities.sort(key=lambda x: x[0], reverse=True)
        highest_cards_probabilities = cards_probabilities[:13]
        hand = [x[1] for x in highest_cards_probabilities]
        result.append(hand)

    return result

def convert_pnn_output_to_symbol(pnn_output):
    suit_order = ['♣', '♦', '♥', '♠', 'NT']
    value_order = ['1', '2', '3', '4', '5', '6', '7']
    bids = []

    for value in value_order:
        for suit in suit_order:
            bids.append(value + suit)

    bids.extend(['P', 'D', 'R'])

    result = []

    for output in pnn_output:
        assert len(output) == 38
        index = output.argmax()
        bid = bids[index]
        result.append(bid)

    return result

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Deep bridge bidding')
    parser.add_argument('command', choices=['train', 'test', 'predict'])
    parser.add_argument('model', choices=['enn', 'pnn'])
    parser.add_argument('--new-model', dest='new_model', action='store_true', help='create new model for training')
    parser.add_argument('--load-model', dest='new_model', action='store_false', help='continue training from existing model')
    parser.add_argument('--input', help='input file for predicting')
    parser.set_defaults(new_model=True)
    args = parser.parse_args()

    fetcher = DataFetcher('split_data/')
    model_filename = 'model_{}.hdf5'.format(args.model)

    if args.command == 'train':
        if args.new_model:
            print('Creating new model')
            if args.model == 'enn':
                model = create_enn_model()
            elif args.model == 'pnn':
                model = create_pnn_model()
        else:
            print('Load existing model "{}"'.format(model_filename))
            model = load_model(model_filename)

        train(fetcher, model, model_type=args.model, model_filename=model_filename, batch_size=4096, epochs=10)
    elif args.command == 'test':
        model = load_model(model_filename)
        test(fetcher, model, model_type=args.model, model_filename=model_filename, batch_size=4096)
    elif args.command == 'predict':
        model = load_model(model_filename)
        result = predict(fetcher, args.input, model, model_type=args.model, model_filename=model_filename, batch_size=4096)
        print(result)

if __name__ == '__main__':
    main()
